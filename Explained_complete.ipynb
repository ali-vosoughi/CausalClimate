{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not import packages for CMIknn and GPDC estimation\n",
      "Could not import packages for CMIknn and GPDC estimation\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_0_mean_1_std(inp_series):\n",
    "    inp_series=inp_series.copy()\n",
    "    mean_ts=np.array([inp_series.mean(axis=1)]).transpose()\n",
    "    mean_ts_mtrx = mean_ts*np.ones((1,inp_series.shape[1]));\n",
    "    unb_data_mtrx = inp_series - mean_ts_mtrx\n",
    "    p = np.power(unb_data_mtrx,2)\n",
    "    s=np.array([p.sum(axis=1)]).transpose()\n",
    "    sc=np.sqrt(s/p.shape[1])\n",
    "    sc2=sc*(np.ones((1,p.shape[1])))\n",
    "    nrm= np.divide(unb_data_mtrx,sc2)\n",
    "    return nrm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 2: Get data from the file and encode the labels using LabelEncoder class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multivariate_split(X,ar_order, valid_percent=0):\n",
    "    X=X.copy()\n",
    "    TS=np.shape(X)[1]\n",
    "    n_vars=np.shape(X)[0]\n",
    "    val_num=int(valid_percent*TS)\n",
    "    my_data_train=torch.zeros((TS-ar_order-val_num,ar_order,n_vars))\n",
    "    my_data_y_train=torch.zeros((TS-ar_order-val_num,1,n_vars))\n",
    "    my_data_val=torch.zeros((val_num,ar_order,n_vars))\n",
    "    my_data_y_val=torch.zeros((val_num,1,n_vars))\n",
    "    for i in range(TS-ar_order-val_num):\n",
    "        my_data_train[i]=torch.from_numpy(X.transpose()[i:i+ar_order,:])\n",
    "        my_data_y_train[i]=torch.from_numpy(X.transpose()[i+ar_order,:])\n",
    "\n",
    "    for i in range(TS-ar_order-val_num, TS-ar_order,1):\n",
    "        my_data_val[i-(TS-ar_order-val_num)]=torch.from_numpy(X.transpose()[i:i+ar_order,:])\n",
    "        my_data_y_val[i-(TS-ar_order-val_num)]=torch.from_numpy(X.transpose()[i+ar_order,:])\n",
    "    return my_data_train, my_data_y_train, my_data_val, my_data_y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_f_stat(RSS_R,RSS_U,n,pu,pr):\n",
    "    f_GC = ((RSS_R-RSS_U)/(RSS_U))*((n-pu-1)/(pu-pr));\n",
    "    return f_GC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I am using in for Granger causality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "\n",
    "ar_order=2\n",
    "\n",
    "\n",
    "in_data_name='datasets/7SYNTHETICS/logistic_1.mat'\n",
    "\n",
    "GroundTruth=[loadmat(in_data_name)['Adj'] for i in range(50)]\n",
    "ts_logistic=[loadmat(in_data_name)['pt_N'][i,:,-500:] for i in range(50)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_normalized=normalize_0_mean_1_std(inp_series=ts_logistic[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Split the data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([498, 4])\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train , X_test, Y_test=multivariate_split(X=X_normalized,ar_order=ar_order)\n",
    "\n",
    "X_train=torch.flatten(X_train, start_dim=1)\n",
    "X_test=torch.flatten(X_test, start_dim=1)\n",
    "\n",
    "Y_train=torch.flatten(Y_train, start_dim=1)\n",
    "Y_test=torch.flatten(Y_test, start_dim=1)\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Step 4: Obtain $k_f$ number of cluster centers with k-means clustering. It has dimensions of $ \\in k_f \\times Nd$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 4)\n"
     ]
    }
   ],
   "source": [
    "k_f=3\n",
    "km= KMeans(n_clusters= k_f, max_iter= 100, random_state=123)\n",
    "km.fit(X_train)\n",
    "cent= km.cluster_centers_\n",
    "\n",
    "print(cent.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Set the width of the kernel as the mean distance between cluster centers. (I select the max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2204506596243991\n"
     ]
    }
   ],
   "source": [
    "max=0 \n",
    "\n",
    "for i in range(k_f):\n",
    "    for j in range(k_f):\n",
    "        d= np.linalg.norm(cent[i]-cent[j])\n",
    "        if(d> max):\n",
    "            max= d\n",
    "d= max\n",
    "\n",
    "sigma= d/math.sqrt(2*k_f)\n",
    "print(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maxg=0 \n",
    "\n",
    "# for ii in range(k_g):\n",
    "#     for jj in range(k_g):\n",
    "#         dg= np.linalg.norm(cent_W_s[ii]-cent_W_s[jj])\n",
    "#         if(dg> max):\n",
    "#             maxg= dg\n",
    "# dg= maxg\n",
    "\n",
    "# sigmag= dg/math.sqrt(2*k_g)\n",
    "# print(sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Iterate through all $N$ time-series from 1 to $n$ where $n \\in N$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_d=np.zeros((np.shape(X_normalized)[0],np.shape(X_normalized)[0]));\n",
    "sig=np.zeros((np.shape(X_normalized)[0],np.shape(X_normalized)[0]));\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(X_normalized.shape[0]):\n",
    "    Z_temp=X_normalized.copy()\n",
    "    Z_train, Z_train_label , _ , _=multivariate_split(X=Z_temp,ar_order=ar_order)\n",
    "    Z_train=torch.flatten(Z_train, start_dim=1)\n",
    "    Z_train_label=torch.flatten(Z_train_label, start_dim=1)\n",
    "    \n",
    "    # Obtain phase space Z_s by exclusing time series of of x_s\n",
    "    Z_s_train, Z_s_train_label , _ , _=multivariate_split(X=np.delete(Z_temp,[i],axis=0),ar_order=ar_order)\n",
    "    # Obtain phase space reconstruction of x_s\n",
    "    W_s_train, W_s_train_label , _ , _=multivariate_split(X=np.array([Z_temp[i]]),ar_order=ar_order)\n",
    "    \n",
    "    # Flatten data\n",
    "    Z_s_train=torch.flatten(Z_s_train, start_dim=1)\n",
    "    Z_s_train_label=torch.flatten(Z_s_train_label, start_dim=1)\n",
    "\n",
    "    W_s_train=torch.flatten(W_s_train, start_dim=1)\n",
    "    W_s_train_label=torch.flatten(W_s_train_label, start_dim=1)\n",
    "    # Obtain k_g number of cluster centers in the phase space W_s with k-means clustering, will have dim=(k_g * d)\n",
    "    k_g=2\n",
    "    kmg= KMeans(n_clusters= k_g, max_iter= 100, random_state=123)\n",
    "    kmg.fit(W_s_train)\n",
    "    cent_W_s= kmg.cluster_centers_\n",
    "    # Calculate activations for each of the k_g neurons\n",
    "    shape= W_s_train.shape\n",
    "    row= shape[0]\n",
    "    column= k_g\n",
    "    G= np.empty((row,column), dtype= float)\n",
    "    maxg=0 \n",
    "\n",
    "    for ii in range(k_g):\n",
    "        for jj in range(k_g):\n",
    "            dg= np.linalg.norm(cent_W_s[ii]-cent_W_s[jj])\n",
    "            if(dg> maxg):\n",
    "                maxg= dg\n",
    "    dg= maxg\n",
    "\n",
    "    sigmag= dg/math.sqrt(2*k_g)\n",
    "    for ii in range(row):\n",
    "        for jj in range(column):\n",
    "            dist= np.linalg.norm(W_s_train[ii]-cent_W_s[jj])\n",
    "            G[ii][jj]= math.exp(-math.pow(dist,2)/math.pow(2*sigmag,2))\n",
    "    # Generalized radial basis function\n",
    "    g_ws=np.array([G[ii]/sum(G[ii]) for ii in range(len(G))])\n",
    "    # Calculate activations for each of the k_f neurons \n",
    "    shape= Z_s_train.shape\n",
    "    row= shape[0]\n",
    "    column= k_f\n",
    "    F= np.empty((row,column), dtype= float)\n",
    "    for ii in range(row):\n",
    "        for jj in range(column):\n",
    "            cent_temp=cent.copy()\n",
    "            cent_temp=np.delete(cent_temp,np.arange(jj,jj+ar_order),axis=1)\n",
    "            dist= np.linalg.norm(Z_s_train[ii]-cent_temp)\n",
    "            F[ii][jj]= math.exp(-math.pow(dist,2)/math.pow(2*sigma,2))\n",
    "    # Generalized radial basis function\n",
    "    f_zs=np.array([F[ii]/sum(F[ii]) for ii in range(len(F))])\n",
    "    \n",
    "    # Prediction in the presence of x_s\n",
    "    dims=np.max([k_f,k_g])\n",
    "    num_samples=f_zs.shape[0]\n",
    "    f_new=np.ones((num_samples,dims))\n",
    "    g_new=np.ones((num_samples,dims))\n",
    "\n",
    "    for ii in range(f_new.shape[0]):\n",
    "        f_new[ii,:f_zs.shape[1]]=f_zs[ii,:]\n",
    "        g_new[ii,:g_ws.shape[1]]=g_ws[ii,:]\n",
    "    fg_combined=np.concatenate((g_new,f_new),axis=1)\n",
    "\n",
    "    GTG= np.dot(fg_combined.T,fg_combined)\n",
    "    GTG_inv= np.linalg.inv(GTG)\n",
    "    fac= np.dot(GTG_inv,fg_combined.T)\n",
    "    W_presence= np.dot(fac,Z_train_label)\n",
    "\n",
    "    prediction_presence= np.dot(fg_combined,W_presence)\n",
    "    error_presence=prediction_presence-np.array(Z_train_label)\n",
    "#     dims=k_f+k_g\n",
    "#     num_samples=f_zs.shape[0]\n",
    "#     f_new=np.zeros((num_samples,dims))\n",
    "\n",
    "#     for ii in range(f_new.shape[0]):\n",
    "#         f_new[ii,:k_f]=f_zs[ii,:]\n",
    "#         f_new[ii,k_f:]=g_ws[ii,:]\n",
    "\n",
    "#     GTG= np.dot(f_new.T,f_new)\n",
    "#     GTG_inv= np.linalg.inv(GTG)\n",
    "#     fac= np.dot(GTG_inv,f_new.T)\n",
    "#     W_presence= np.dot(fac,Z_train_label)\n",
    "\n",
    "#     prediction_presence= np.dot(f_new,W_presence)\n",
    "#     error_presence=prediction_presence-np.array(Z_train_label)\n",
    "    sig[i,:]=np.diag(np.cov(error_presence.T))\n",
    "    \n",
    "    # Prediction without x_s\n",
    "    GTG= np.dot(f_zs.T,f_zs)\n",
    "    GTG_inv= np.linalg.inv(GTG)\n",
    "    fac= np.dot(GTG_inv,f_zs.T)\n",
    "    W_absence= np.dot(fac,Z_train_label)\n",
    "\n",
    "    prediction_absence= np.dot(f_zs,W_absence)\n",
    "    error_absence=prediction_absence-np.array(Z_train_label)\n",
    "    sig_d[i,:]=np.diag(np.cov(error_absence.T))\n",
    "# Comupte the Granger causality index\n",
    "\n",
    "Aff=np.log(np.divide(sig_d,sig))\n",
    "Aff=(Aff>0)*Aff\n",
    "np.fill_diagonal(Aff,0)\n",
    "Aff\n",
    "\n",
    "f_stat=calc_f_stat(sig_d, sig, n=num_samples+1, pu=k_f+k_g, pr=k_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_new=np.concatenate((f_zs,g_ws),axis=1)\n",
    "\n",
    "GTG= np.dot(f_new.T,f_new)\n",
    "GTG_inv= np.linalg.inv(GTG)\n",
    "fac= np.dot(GTG_inv,f_new.T)\n",
    "W_presence= np.dot(fac,Z_train_label)\n",
    "\n",
    "prediction_presence= np.dot(f_new,W_presence)\n",
    "error_presence=prediction_presence-np.array(Z_train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 2)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_presence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1=np.concatenate((f_zs, np.zeros_like(g_ws)),axis=1)\n",
    "A2=np.concatenate((np.zeros_like(f_zs), g_ws),axis=1)\n",
    "AA=np.concatenate((A1,A2),axis=0)\n",
    "fg_new=np.concatenate((f_zs,g_ws),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "AA=np.concatenate((A1,A2),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False,  True],\n",
       "       [False, False]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.fill_diagonal(f_stat,0)\n",
    "f_stat>0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(Aff>0.00)*1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GroundTruth[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multivariate_split(X,ar_order, valid_percent=0):\n",
    "    X=X.copy()\n",
    "    TS=np.shape(X)[1]\n",
    "    n_vars=np.shape(X)[0]\n",
    "    val_num=int(valid_percent*TS)\n",
    "    my_data_train=torch.zeros((TS-ar_order-val_num,ar_order,n_vars))\n",
    "    my_data_y_train=torch.zeros((TS-ar_order-val_num,1,n_vars))\n",
    "    my_data_val=torch.zeros((val_num,ar_order,n_vars))\n",
    "    my_data_y_val=torch.zeros((val_num,1,n_vars))\n",
    "    for i in range(TS-ar_order-val_num):\n",
    "        my_data_train[i]=torch.from_numpy(X.transpose()[i:i+ar_order,:])\n",
    "        my_data_y_train[i]=torch.from_numpy(X.transpose()[i+ar_order,:])\n",
    "\n",
    "    for i in range(TS-ar_order-val_num, TS-ar_order,1):\n",
    "        my_data_val[i-(TS-ar_order-val_num)]=torch.from_numpy(X.transpose()[i:i+ar_order,:])\n",
    "        my_data_y_val[i-(TS-ar_order-val_num)]=torch.from_numpy(X.transpose()[i+ar_order,:])\n",
    "    return my_data_train, my_data_y_train, my_data_val, my_data_y_val\n",
    "\n",
    "\n",
    "def calc_f_stat(RSS_R,RSS_U,n,pu,pr):\n",
    "    f_GC = ((RSS_R-RSS_U)/(RSS_U))*((n-pu-1)/(pu-pr));\n",
    "    return f_GC\n",
    "\n",
    "def lsNGC(inp_series, ar_order=1, k_f=3, k_g=2, normalize=1):\n",
    "    if normalize:\n",
    "        X_normalized=normalize_0_mean_1_std(inp_series)\n",
    "\n",
    "    km= KMeans(n_clusters= k_f, max_iter= 100, random_state=123)\n",
    "    km.fit(X_train)\n",
    "    cent= km.cluster_centers_\n",
    "\n",
    "\n",
    "    max=0 \n",
    "\n",
    "    for i in range(k_f):\n",
    "        for j in range(k_f):\n",
    "            d= np.linalg.norm(cent[i]-cent[j])\n",
    "            if(d> max):\n",
    "                max= d\n",
    "    d= max\n",
    "\n",
    "    sigma= d/math.sqrt(2*k_f)\n",
    "\n",
    "    sig_d=np.zeros((np.shape(X_normalized)[0],np.shape(X_normalized)[0]));\n",
    "    sig=np.zeros((np.shape(X_normalized)[0],np.shape(X_normalized)[0]));\n",
    "\n",
    "\n",
    "    for i in range(X_normalized.shape[0]):\n",
    "        Z_temp=X_normalized.copy()\n",
    "        Z_train, Z_train_label , _ , _=multivariate_split(X=Z_temp,ar_order=ar_order)\n",
    "        Z_train=torch.flatten(Z_train, start_dim=1)\n",
    "        Z_train_label=torch.flatten(Z_train_label, start_dim=1)\n",
    "\n",
    "        # Obtain phase space Z_s by exclusing time series of of x_s\n",
    "        Z_s_train, Z_s_train_label , _ , _=multivariate_split(X=np.delete(Z_temp,[i],axis=0),ar_order=ar_order)\n",
    "        # Obtain phase space reconstruction of x_s\n",
    "        W_s_train, W_s_train_label , _ , _=multivariate_split(X=np.array([Z_temp[i]]),ar_order=ar_order)\n",
    "\n",
    "        # Flatten data\n",
    "        Z_s_train=torch.flatten(Z_s_train, start_dim=1)\n",
    "        Z_s_train_label=torch.flatten(Z_s_train_label, start_dim=1)\n",
    "\n",
    "        W_s_train=torch.flatten(W_s_train, start_dim=1)\n",
    "        W_s_train_label=torch.flatten(W_s_train_label, start_dim=1)\n",
    "        # Obtain k_g number of cluster centers in the phase space W_s with k-means clustering, will have dim=(k_g * d)\n",
    "        kmg= KMeans(n_clusters= k_g, max_iter= 100, random_state=123)\n",
    "        kmg.fit(W_s_train)\n",
    "        cent_W_s= kmg.cluster_centers_\n",
    "        # Calculate activations for each of the k_g neurons\n",
    "        shape= W_s_train.shape\n",
    "        row= shape[0]\n",
    "        column= k_g\n",
    "        G= np.empty((row,column), dtype= float)\n",
    "        maxg=0 \n",
    "\n",
    "        for ii in range(k_g):\n",
    "            for jj in range(k_g):\n",
    "                dg= np.linalg.norm(cent_W_s[ii]-cent_W_s[jj])\n",
    "                if(dg> maxg):\n",
    "                    maxg= dg\n",
    "        dg= maxg\n",
    "\n",
    "        sigmag= dg/math.sqrt(2*k_g)\n",
    "        for ii in range(row):\n",
    "            for jj in range(column):\n",
    "                dist= np.linalg.norm(W_s_train[ii]-cent_W_s[jj])\n",
    "                G[ii][jj]= math.exp(-math.pow(dist,2)/math.pow(2*sigmag,2))\n",
    "        # Generalized radial basis function\n",
    "        g_ws=np.array([G[ii]/sum(G[ii]) for ii in range(len(G))])\n",
    "        # Calculate activations for each of the k_f neurons \n",
    "        shape= Z_s_train.shape\n",
    "        row= shape[0]\n",
    "        column= k_f\n",
    "        F= np.empty((row,column), dtype= float)\n",
    "        for ii in range(row):\n",
    "            for jj in range(column):\n",
    "                cent_temp=cent.copy()\n",
    "                cent_temp=np.delete(cent_temp,np.arange(jj,jj+ar_order),axis=1)\n",
    "                dist= np.linalg.norm(Z_s_train[ii]-cent_temp)\n",
    "                F[ii][jj]= math.exp(-math.pow(dist,2)/math.pow(2*sigma,2))\n",
    "        # Generalized radial basis function\n",
    "        f_zs=np.array([F[ii]/sum(F[ii]) for ii in range(len(F))])\n",
    "\n",
    "        # Prediction in the presence of x_s\n",
    "        dims=np.max([k_f,k_g])\n",
    "        num_samples=f_zs.shape[0]\n",
    "        f_new=np.ones((num_samples,dims))\n",
    "        g_new=np.ones((num_samples,dims))\n",
    "\n",
    "        for ii in range(f_new.shape[0]):\n",
    "            f_new[ii,:f_zs.shape[1]]=f_zs[ii,:]\n",
    "            g_new[ii,:g_ws.shape[1]]=g_ws[ii,:]\n",
    "        fg_combined=np.concatenate((g_new,f_new),axis=1)\n",
    "\n",
    "        GTG= np.dot(fg_combined.T,fg_combined)\n",
    "        GTG_inv= np.linalg.inv(GTG)\n",
    "        fac= np.dot(GTG_inv,fg_combined.T)\n",
    "        W_presence= np.dot(fac,Z_train_label)\n",
    "\n",
    "        prediction_presence= np.dot(fg_combined,W_presence)\n",
    "        error_presence=prediction_presence-np.array(Z_train_label)\n",
    "        sig[i,:]=np.diag(np.cov(error_presence.T))\n",
    "\n",
    "        # Prediction without x_s\n",
    "        GTG= np.dot(f_zs.T,f_zs)\n",
    "        GTG_inv= np.linalg.inv(GTG)\n",
    "        fac= np.dot(GTG_inv,f_zs.T)\n",
    "        W_absence= np.dot(fac,Z_train_label)\n",
    "\n",
    "        prediction_absence= np.dot(f_zs,W_absence)\n",
    "        error_absence=prediction_absence-np.array(Z_train_label)\n",
    "        sig_d[i,:]=np.diag(np.cov(error_absence.T))\n",
    "    # Comupte the Granger causality index\n",
    "\n",
    "    Aff=np.log(np.divide(sig_d,sig))\n",
    "    Aff=(Aff>0)*Aff\n",
    "    np.fill_diagonal(Aff,0)\n",
    "    f_stat=calc_f_stat(sig_d, sig, n=num_samples+1, pu=k_f+k_g, pr=k_f)\n",
    "    np.fill_diagonal(f_stat,0)\n",
    "    \n",
    "    return Aff, f_stat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
